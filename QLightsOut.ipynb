{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLearning - LightsOut (3x3, 4x4 o 5x5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importación de Bibliotecas y Definición de Parámetros\n",
    "\n",
    "Empezamos importando las bibliotecas necesarias y estableciendo algunos parámetros iniciales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import product\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "rows, columns = 4, 4\n",
    "states = rows * columns\n",
    "on, off = 1, 0\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones de Conversión de Estado y Visualización\n",
    "Luego, se definen algunas funciones auxiliares para convertir entre las coordenadas (x, y) y el estado, así como para visualizar el tablero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_from_xy(x, y):\n",
    "    if isinstance(x, int):\n",
    "        return x\n",
    "    return y * columns + x\n",
    "\n",
    "def xy_from_state(state):\n",
    "    return divmod(state, columns)\n",
    "\n",
    "def all_possible_states():\n",
    "    return np.array(list(product([0, 1], repeat=(rows ** 2))))\n",
    "\n",
    "def view3x3(state):\n",
    "    return f\"{state[0]} {state[1]} {state[2]}\\n{state[3]} {state[4]} {state[5]}\\n{state[6]} {state[7]} {state[8]}\"\n",
    "\n",
    "def view4x4(state):\n",
    "    return f\"{state[0]} {state[1]} {state[2]} {state[3]}\\n{state[4]} {state[5]} {state[6]} {state[7]}\\n{state[8]} {state[9]} {state[10]} {state[11]}\\n{state[12]} {state[13]} {state[14]} {state[15]}\"\n",
    "\n",
    "def view5x5(state):\n",
    "    return f\"{state[0]} {state[1]} {state[2]} {state[3]} {state[4]}\\n{state[5]} {state[6]} {state[7]} {state[8]} {state[9]}\\n{state[10]} {state[11]} {state[12]} {state[13]} {state[14]}\\n{state[15]} {state[16]} {state[17]} {state[18]} {state[19]}\\n{state[20]} {state[21]} {state[22]} {state[23]} {state[24]}\"\n",
    "\n",
    "def view_board(state):\n",
    "    if rows == 3:\n",
    "        return view3x3(state)\n",
    "    elif rows == 4:\n",
    "        return view4x4(state)\n",
    "    elif rows == 5:\n",
    "        return view5x5(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas funciones son útiles para convertir entre las coordenadas del estado y para visualizar el tablero en una cuadrícula de 3x3, 4x4 o 5x5, dependiendo del tamaño especificado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones Auxiliares y Barras de Progreso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La función loading_bar se utiliza para visualizar una barra de progreso durante el entrenamiento y la evaluación.\n",
    "* perform_action se encarga de realizar una acción en el estado actual del juego.\n",
    "* calculate_reward calcula la recompensa actual basada en cuántas luces están apagadas en el estado actual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_bar(n_epochs):\n",
    "    print(f'\\r[{\"#\" * (n_epochs // 2)}{\" \" * (50 - n_epochs // 2)}] {n_epochs}% ', end='')\n",
    "\n",
    "def perform_action(state, n_state):\n",
    "    new_state = state.copy()\n",
    "    row_size = rows\n",
    "\n",
    "    new_state[n_state] = on if new_state[n_state] == off else off\n",
    "    if n_state - row_size >= 0:\n",
    "        new_state[n_state - row_size] = on if new_state[n_state - row_size] == off else off\n",
    "    if n_state + row_size < (rows ** 2):\n",
    "        new_state[n_state + row_size] = on if new_state[n_state + row_size] == off else off\n",
    "    if n_state - 1 >= 0 and n_state % row_size != 0:\n",
    "        new_state[n_state - 1] = on if new_state[n_state - 1] == off else off\n",
    "    if n_state + 1 < (rows ** 2) and (n_state + 1) % row_size != 0:\n",
    "        new_state[n_state + 1] = on if new_state[n_state + 1] == off else off\n",
    "\n",
    "    return new_state\n",
    "\n",
    "def calculate_reward(state):\n",
    "    lights_off = 0\n",
    "    for i in state:\n",
    "        if i == 0:\n",
    "            lights_off += 1\n",
    "    return lights_off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo de Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La función Q_learning implementa el algoritmo de Q-Learning para aprender una política óptima.\n",
    "* Se inicializa una matriz Q con ceros y se actualiza iterativamente con las recompensas obtenidas por las acciones tomadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning(states, max_steps, episodes):\n",
    "    b = rows ** 2\n",
    "    a = 2 ** b\n",
    "    Q = np.zeros((a, b))\n",
    "    alpha = 0.1\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state_index = random.randint(0, b - 1)\n",
    "        state = states[state_index]\n",
    "        for step in range(max_steps):\n",
    "            if random.uniform(0, 1) < 0.5:\n",
    "                action_taken = random.randint(0, b - 1)\n",
    "            else:\n",
    "                action_taken = np.argmax(Q[state_index])\n",
    "\n",
    "            next_state = perform_action(state, action_taken)\n",
    "            reward_next_state = calculate_reward(next_state)\n",
    "            Q[state_index, action_taken] += alpha * (reward_next_state + gamma * np.max(Q[next_state]) - Q[state_index, action_taken])\n",
    "            alpha = 1 / np.sqrt(episode + 1)\n",
    "            state = next_state\n",
    "        loading_bar(episode * 100 // episodes)\n",
    "    return Q\n",
    "\n",
    "states = all_possible_states()[1:]\n",
    "\n",
    "Q = Q_learning(states, 500, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación del Agente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* evaluate_agent evalúa el desempeño del agente entrenado utilizando el Q-table aprendido.\n",
    "* Se realizan múltiples episodios para calcular el promedio de las recompensas obtenidas y su desviación estándar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(states, Q, max_steps, episodes):\n",
    "    episode_rewards = []\n",
    "    b = rows ** 2\n",
    "    for episode in range(episodes):\n",
    "        state_index = random.randint(0, b - 1)\n",
    "        state = states[state_index]\n",
    "        total_rewards_ep = 0\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            action_taken = np.argmax(Q[state_index])\n",
    "            next_state = perform_action(state, action_taken)\n",
    "            reward_next_state = calculate_reward(next_state)\n",
    "            total_rewards_ep += reward_next_state\n",
    "            state_index = np.argmax(Q[next_state])  # Update the state index for the next step\n",
    "\n",
    "        episode_rewards.append(total_rewards_ep)\n",
    "        loading_bar(episode * 100 // episodes)\n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "    return mean_reward, std_reward\n",
    "\n",
    "mean_reward, std_reward = evaluate_agent(states, Q, 500, 500)\n",
    "print()\n",
    "print(f\"Mean Reward: {mean_reward}, Std Reward: {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulación del Juego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* simulate_game simula un juego del problema de luces apagadas utilizando el Q-table aprendido.\n",
    "* El juego comienza con un estado inicial específico y el agente selecciona acciones utilizando una política greedy basada en los valores de Q aprendidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_game(Q):\n",
    "    # Start with a random initial state\n",
    "    b = rows ** 2\n",
    "    actions = []\n",
    "    initial_state = [1, 1, 0, 0,\n",
    "                     1, 1, 1, 0,\n",
    "                     0, 0, 1, 1,\n",
    "                     1, 1, 0, 1,]\n",
    "    current_state = initial_state.copy()\n",
    "\n",
    "    print(\"Initial State:\")\n",
    "    print(view_board(current_state))\n",
    "\n",
    "    while True:\n",
    "        # Choose action based on learned Q-values (greedy policy)\n",
    "        if random.uniform(0, 1):\n",
    "            action_taken = random.randint(0, b - 1)\n",
    "        else:\n",
    "            action_taken = np.argmax(Q[state_from_xy(*xy_from_state(current_state))])\n",
    "\n",
    "        # Perform the selected action\n",
    "        current_state = perform_action(current_state, action_taken)\n",
    "\n",
    "        actions += [action_taken]\n",
    "\n",
    "        # Check if all lights are turned off\n",
    "        if calculate_reward(current_state) == 0:\n",
    "            print(\"Final State:\")\n",
    "            print(view_board(current_state))\n",
    "            print(\"Congratulations! All lights are turned off.\")\n",
    "            return actions\n",
    "            break\n",
    "\n",
    "        if len(actions) > 10:\n",
    "          current_state = initial_state\n",
    "          actions = []\n",
    "\n",
    "    return 0\n",
    "\n",
    "print(simulate_game(Q))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
